{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (VAE) Implementation on CIFAR-10\n",
    "\n",
    "This notebook implements a complete Variational Autoencoder for image reconstruction using PyTorch. The implementation includes:\n",
    "- An encoder network that maps input images to a latent space distribution\n",
    "- A decoder network that reconstructs images from latent representations\n",
    "- Training loop with loss visualization and model checkpointing\n",
    "- Utility functions for saving reconstructed images and plotting training curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for our VAE implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Class\n",
    "\n",
    "Define all hyperparameters and configuration settings for the VAE model in a centralized configuration class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Network\n",
    "\n",
    "The encoder network takes input images and maps them to a latent space. It outputs two vectors:\n",
    "- **mu**: Mean of the latent distribution\n",
    "- **logvar**: Log variance of the latent distribution\n",
    "\n",
    "The encoder uses three convolutional layers with batch normalization and ReLU activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=Config.in_channels, out_channels=Config.encoder_channels[0], kernel_size=Config.kernel_size,\n",
    "                     stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(Config.encoder_channels[0]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=Config.encoder_channels[0], out_channels=Config.encoder_channels[1], kernel_size=Config.kernel_size,\n",
    "                     stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(Config.encoder_channels[1]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=Config.encoder_channels[1], out_channels=Config.encoder_channels[2], kernel_size=Config.kernel_size,\n",
    "                     stride=2, padding=1),\n",
    "            nn.BatchNorm2d(Config.encoder_channels[2]),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.mu = nn.Linear(in_features=Config.encoder_channels[2] * 4 * 4, out_features=Config.latent_dim)\n",
    "        self.logvar = nn.Linear(in_features=Config.encoder_channels[2] * 4 * 4, out_features=Config.latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flatten(x)\n",
    "        mu = self.mu(x)\n",
    "        logvar = self.logvar(x)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Network\n",
    "\n",
    "The decoder network reconstructs images from the latent space representation. It uses:\n",
    "- A fully connected layer to expand the latent vector to the required size\n",
    "- Transposed convolutions to upsample the feature maps back to the original image size\n",
    "- Tanh activation at the output to ensure pixel values are in the range [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_features=Config.latent_dim, out_features=Config.decoder_channels[0]*4*4)\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=Config.decoder_channels[0], out_channels=Config.decoder_channels[1],\n",
    "                             kernel_size=Config.kernel_size, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(Config.decoder_channels[1]),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=Config.decoder_channels[1], out_channels=Config.decoder_channels[2],\n",
    "                             kernel_size=Config.kernel_size, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(Config.decoder_channels[2]),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=Config.decoder_channels[2], out_channels=Config.out_channels,\n",
    "                             kernel_size=Config.kernel_size, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)  # [B,latent_dim] -> [B,128*4*4] Converts the input to a 1-D tensor\n",
    "        x = x.view(-1, Config.decoder_channels[0], 4, 4)  # Reshapes the Input_tensor back to 4-D tensor\n",
    "        x = self.deconv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete VAE Model\n",
    "\n",
    "The VAE combines the encoder and decoder with a reparameterization trick. The reparameterization trick allows us to sample from the latent distribution while maintaining the ability to backpropagate gradients.\n",
    "\n",
    "**Reparameterization Trick**: Instead of sampling directly from N(μ, σ²), we sample ε ~ N(0,1) and compute z = μ + σ × ε"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Helper functions for saving reconstructed images during training and plotting the training loss curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reconstructed_image(model, dataloader, epoch, max_samples=8):\n",
    "    \"\"\"Save reconstructed images during training for visualization\"\"\"\n",
    "    model.eval()\n",
    "    os.makedirs(Config.reconstruction_save_path, exist_ok=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        images = next(iter(dataloader))[0][:max_samples].to(Config.device)\n",
    "        recon, _, _ = model(images)\n",
    "        \n",
    "        # Combine them into GRIDS\n",
    "        grid = make_grid(recon, nrow=max_samples, normalize=True)\n",
    "        file_path = os.path.join(Config.reconstruction_save_path, f\"recon_epoch:{epoch}.png\")\n",
    "        save_image(grid, file_path)\n",
    "    \n",
    "    model.train()\n",
    "    return grid\n",
    "\n",
    "def plot_training_curve(loss_list, save_path=\"saves/vae_loss_plot.png\"):\n",
    "    \"\"\"Plot and save the training loss curve\"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(loss_list, label=\"Epoch Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"VAE Training Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training Environment\n",
    "\n",
    "Initialize the device, set random seed for reproducibility, and set up Weights & Biases logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: atharv3105 (atharv3105-dr-a-p-j-abdul-kalam-technical-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Atharva\\Projects\\Image_Generation_VAE+GAN\\vae\\wandb\\run-20250718_133542-tourm5kh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/atharv3105-dr-a-p-j-abdul-kalam-technical-university/VAE-CIFAR-10/runs/tourm5kh' target=\"_blank\">vae_run_2</a></strong> to <a href='https://wandb.ai/atharv3105-dr-a-p-j-abdul-kalam-technical-university/VAE-CIFAR-10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/atharv3105-dr-a-p-j-abdul-kalam-technical-university/VAE-CIFAR-10' target=\"_blank\">https://wandb.ai/atharv3105-dr-a-p-j-abdul-kalam-technical-university/VAE-CIFAR-10</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/atharv3105-dr-a-p-j-abdul-kalam-technical-university/VAE-CIFAR-10/runs/tourm5kh' target=\"_blank\">https://wandb.ai/atharv3105-dr-a-p-j-abdul-kalam-technical-university/VAE-CIFAR-10/runs/tourm5kh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1,120,259\n"
     ]
    }
   ],
   "source": [
    "# Set Device & SEED\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(Config.seed)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize WandB & Model\n",
    "wandb.init(project=Config.wandb_project,\n",
    "           name=f\"{Config.wandb_run_name}_2\",\n",
    "           config={k: v for k, v in Config.__dict__.items() if not k.startswith(\"__\")})\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)\n",
    "reconstruction_criterion = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "The VAE loss function consists of two components:\n",
    "1. **Reconstruction Loss**: Measures how well the decoder reconstructs the input (using MSE)\n",
    "2. **KL Divergence**: Regularizes the latent space to follow a standard normal distribution\n",
    "\n",
    "Total Loss = Reconstruction Loss + KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    \"\"\"Calculate VAE loss with reconstruction and KL divergence components\"\"\"\n",
    "    recon_loss = reconstruction_criterion(recon_x, x)\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    total_loss = recon_loss + kl_divergence\n",
    "    return total_loss, recon_loss, kl_divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load the CIFAR-10 dataset with appropriate transformations. We normalize the images to [-1, 1] range to match the Tanh output of the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset & Make DataLoader\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=Config.dataset_path, train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Number of batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Main training loop that:\n",
    "1. Performs forward pass through the VAE\n",
    "2. Calculates reconstruction and KL divergence losses\n",
    "3. Performs backpropagation and parameter updates\n",
    "4. Logs metrics to Weights & Biases\n",
    "5. Saves reconstructed images and model checkpoints at specified intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for saving\n",
    "os.makedirs(\"saves/reconstructions\", exist_ok=True)\n",
    "os.makedirs(\"saves/checkpoints\", exist_ok=True)\n",
    "\n",
    "# Training Loop\n",
    "total_loss_list = []\n",
    "model.train()\n",
    "\n",
    "for epoch in range(1, Config.num_epochs + 1):\n",
    "    epoch_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch: [{epoch}/{Config.num_epochs}]\", leave=False)\n",
    "    \n",
    "    for x, _ in loop:\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        recon_x, mu, logvar = model(x)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss, recon_loss, kl_loss = loss_fn(recon_x, x, mu, logvar)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item() / x.size(0))\n",
    "        \n",
    "        # Log batch metrics\n",
    "        wandb.log({\n",
    "            \"Batch Loss\": loss.item() / x.size(0),\n",
    "            \"Reconstruction Loss\": recon_loss.item() / x.size(0),\n",
    "            \"KL Loss\": kl_loss.item() / x.size(0),\n",
    "        })\n",
    "    \n",
    "    # Calculate average epoch loss\n",
    "    avg_loss = epoch_loss / len(train_loader.dataset)\n",
    "    total_loss_list.append(avg_loss)\n",
    "    wandb.log({\"Epoch Loss\": avg_loss, \"Epoch\": epoch})\n",
    "    \n",
    "    # Save Reconstruction\n",
    "    if epoch % Config.save_reconstruction_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x, _ = next(iter(train_loader))\n",
    "            x = x.to(device)[:8]\n",
    "            recon_x, _, _ = model(x)\n",
    "            compare = torch.cat([x, recon_x])\n",
    "            save_image(compare.cpu(), f\"saves/reconstructions/recon_epoch:{epoch}.png\", nrow=8)\n",
    "        model.train()\n",
    "    \n",
    "    # Save Checkpoint\n",
    "    if epoch % 25 == 0:\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optim_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, f\"saves/checkpoints/vae_epoch:{epoch}.pt\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Results\n",
    "\n",
    "Plot the training loss curve and save the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "plot_training_curve(total_loss_list)\n",
    "wandb.save(\"saves/vae_training_loss.png\")\n",
    "\n",
    "print(f\"Final training loss: {total_loss_list[-1]:.6f}\")\n",
    "print(\"Training loss curve saved to saves/vae_training_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Final Reconstructions\n",
    "\n",
    "Generate and save final reconstructions to visualize the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final reconstructions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a batch of test images\n",
    "    test_images, _ = next(iter(train_loader))\n",
    "    test_images = test_images[:8].to(device)\n",
    "    \n",
    "    # Generate reconstructions\n",
    "    reconstructions, mu, logvar = model(test_images)\n",
    "    \n",
    "    # Save comparison\n",
    "    comparison = torch.cat([test_images, reconstructions])\n",
    "    save_image(comparison.cpu(), \"saves/final_reconstruction_comparison.png\", nrow=8)\n",
    "    \n",
    "    print(\"Final reconstructions saved to saves/final_reconstruction_comparison.png\")\n",
    "    print(\"Top row: Original images\")\n",
    "    print(\"Bottom row: Reconstructed images\")\n",
    "\n",
    "# Close wandb run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
